{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Zindi Cigar Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xX2y4aQD7tFX",
        "pyh4ZsYm70lK",
        "0FieP1WYLMR0"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX2y4aQD7tFX"
      },
      "source": [
        "# Upload Data and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyXkjWfV7Sjq",
        "outputId": "a982b994-0c04-4b6e-813f-24870a851123",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from zipfile import ZipFile \n",
        "# specifying the zip file name \n",
        "file_name = \"/content/drive/My Drive/ZINDI_CGIAR/data.zip\"\n",
        "# opening the zip file in READ mode \n",
        "with ZipFile(file_name, 'r') as zi: \n",
        "    # extracting all the files \n",
        "    print('Extracting all the files now...') \n",
        "    zi.extractall() \n",
        "    print('Done!')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting all the files now...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPAzb1pRJ37y",
        "outputId": "a6d34188-4adf-4a66-f499-cc5b1aced761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Oct  6 18:24:15 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0    31W /  70W |   3623MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhtqLNXClKNl"
      },
      "source": [
        "!cp \"/content/drive/My Drive/ZINDI_CGIAR/Train.csv\" . \n",
        "!cp \"/content/drive/My Drive/ZINDI_CGIAR/SampleSubmission.csv\" ."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynh7Tqww3Aig",
        "outputId": "51c4dca8-3373-452d-912a-f4bc24e3d4a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "!pip install efficientnet_pytorch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/83/f9c5f44060f996279e474185ebcbd8dbd91179593bffb9abe3afa55d085b/efficientnet_pytorch-0.7.0.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.18.5)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.0-cp36-none-any.whl size=16031 sha256=180528042b6b8b4cea7190394177bdce9e6e1ea734aa1eefc852d3acebf2e83e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/c6/e1/7a808b26406239712cfce4b5ceeb67d9513ae32aa4b31445c6\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w24RQCaX0Zyi"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from tqdm import tqdm_notebook as tqdm \n",
        "from sklearn.model_selection import train_test_split\n",
        "import albumentations\n",
        "from albumentations import torch as AT\n",
        "import cv2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba854myQBcfU"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "SEED_VAL  = 1000\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "def seed_all(SEED):\n",
        "  random.seed(SEED_VAL)\n",
        "  np.random.seed(SEED_VAL)\n",
        "  torch.manual_seed(SEED_VAL)\n",
        "  torch.cuda.manual_seed_all(SEED_VAL)\n",
        "  os.environ['PYTHONHASHSEED'] = str(SEED_VAL)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyh4ZsYm70lK"
      },
      "source": [
        "# Preparing Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV6u_nW3pc31"
      },
      "source": [
        "def cv_reader(path):\n",
        "  img = cv2.imread(path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  return img\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data_path, df, transform=None,mode='train'):\n",
        "        self.df = df\n",
        "        self.loader = cv_reader\n",
        "        self.transform = transform\n",
        "        self.dir = data_path\n",
        "        self.mode = mode\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_name = self.df.image_name[index]\n",
        "        image = self.loader(os.path.join(self.dir, image_name+'.jpeg'))\n",
        "        if self.transform is not None:\n",
        "          image = self.transform(image=image)\n",
        "          image = image['image']\n",
        "        \n",
        "        \n",
        "\n",
        "        if self.mode == 'train':\n",
        "            label = self.df.target[index]\n",
        "            return {'image' : torch.tensor(image,dtype=torch.float), \n",
        "                'label' : torch.tensor(label,dtype = torch.float) }\n",
        "            \n",
        "        return {'image' : torch.tensor(image,dtype=torch.float), \n",
        "}            \n",
        "        \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njGRGejm2i6D"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self,name):\n",
        "        super(Net, self).__init__()\n",
        "        self.name  = name\n",
        "        if name == 'b0':\n",
        "          self.arch =  EfficientNet.from_pretrained('efficientnet-b0')\n",
        "          self.arch._fc = nn.Linear(in_features=1280, out_features=1, bias=True)\n",
        "        elif name == 'b1':\n",
        "          self.arch =  EfficientNet.from_pretrained('efficientnet-b1')\n",
        "          self.arch._fc = nn.Linear(in_features=1280, out_features=1, bias=True)\n",
        "        elif name == 'b2':\n",
        "          self.arch =  EfficientNet.from_pretrained('efficientnet-b2')\n",
        "          self.arch._fc = nn.Linear(in_features=1408, out_features=1, bias=True)\n",
        "        elif name =='b3':\n",
        "          self.arch =  EfficientNet.from_pretrained('efficientnet-b3')\n",
        "          self.arch._fc = nn.Linear(in_features=1536, out_features=1, bias=True)\n",
        "        elif name =='b4':\n",
        "          self.arch =  EfficientNet.from_pretrained('efficientnet-b4')\n",
        "          self.arch._fc = nn.Linear(in_features=1792, out_features=1, bias=True)\n",
        "        elif name =='b5':\n",
        "          self.arch =  EfficientNet.from_pretrained('efficientnet-b5')\n",
        "          self.arch._fc = nn.Linear(in_features=2048, out_features=1, bias=True)\n",
        "        elif name =='b6':\n",
        "          self.arch =  EfficientNet.from_pretrained('efficientnet-b6')\n",
        "          self.arch._fc = nn.Linear(in_features=2304, out_features=1, bias=True)\n",
        "        elif name =='b7':\n",
        "          self.arch =  EfficientNet.from_pretrained('efficientnet-b7')\n",
        "          self.arch._fc = nn.Linear(in_features=2560, out_features=1, bias=True)\n",
        "        elif name == 'densenet121':\n",
        "          self.arch = models.densenet121(pretrained=True)\n",
        "          num_ftrs = self.arch.classifier.in_features\n",
        "          self.arch.classifier = nn.Linear(num_ftrs,1,bias=True)\n",
        "        elif name == 'densenet161':\n",
        "          self.arch = models.densenet161(pretrained=True)\n",
        "          num_ftrs = self.arch.classifier.in_features\n",
        "          self.arch.classifier = nn.Linear(num_ftrs,1,bias=True)\n",
        "        elif name == 'densenet169':\n",
        "          self.arch = models.densenet169(pretrained=True)\n",
        "          num_ftrs = self.arch.classifier.in_features\n",
        "          self.arch.classifier = nn.Linear(num_ftrs,1,bias=True)\n",
        "        elif name == 'densenet201':\n",
        "          self.arch = models.densenet201(pretrained=True)\n",
        "          num_ftrs = self.arch.classifier.in_features\n",
        "          self.arch.classifier = nn.Linear(num_ftrs,1,bias=True)\n",
        "        elif name == 'resnet50':\n",
        "          self.arch = models.resnet50(pretrained=True)\n",
        "          num_ftrs = self.arch.fc.in_features\n",
        "          self.arch.fc = nn.Linear(num_ftrs,1,bias=True)\n",
        "        elif name == 'resnet101':\n",
        "          self.arch = models.resnet101(pretrained=True)\n",
        "          num_ftrs = self.arch.fc.in_features\n",
        "          self.arch.fc = nn.Linear(num_ftrs,1,bias=True)\n",
        "        elif name == 'resnet152':\n",
        "          self.arch = models.resnet152(pretrained=True)\n",
        "          num_ftrs = self.arch.fc.in_features\n",
        "          self.arch.fc = nn.Linear(num_ftrs,1,bias=True)\n",
        "        elif name == 'resnet34':\n",
        "          self.arch = models.resnet34(pretrained=True)\n",
        "          num_ftrs = self.arch.fc.in_features\n",
        "          self.arch.fc = nn.Linear(num_ftrs,1,bias=True)\n",
        "        elif name == 'resnext101':\n",
        "          self.arch = models.resnext101_32x8d(pretrained=True)\n",
        "          self.arch.drop_out = nn.Dropout(p=0.2)\n",
        "          num_ftrs = self.arch.fc.in_features\n",
        "          self.arch.fc = nn.Linear(num_ftrs,1,bias=True)\n",
        "        elif name == 'resnext50':\n",
        "          self.arch = models.resnext50_32x4d(pretrained=True)\n",
        "          self.arch.drop_out = nn.Dropout(p=0.2)\n",
        "          num_ftrs = self.arch.fc.in_features\n",
        "          self.arch.fc = nn.Linear(num_ftrs,1,bias=True)\n",
        "        elif name == 'mobilenetv2':\n",
        "          self.arch = models.mobilenet_v2(pretrained=True)\n",
        "          self.arch.classifier = nn.Sequential(\n",
        "              nn.Dropout(p=0.1, inplace=False),\n",
        "              nn.Linear(in_features=1280, out_features=1, bias=True),\n",
        "          )\n",
        "\n",
        "        elif name == 'alexnet':\n",
        "          self.arch = models.alexnet(pretrained=False)\n",
        "          self.arch.classifier = nn.Sequential(\n",
        "              \n",
        "                  nn.Dropout(p=0.5, inplace=False),\n",
        "                  nn.Linear(in_features=9216, out_features=1, bias=True)\n",
        "                                                )\n",
        "\n",
        "        elif name =='rexnetv1':\n",
        "            model = rexnetv1.ReXNetV1(width_mult=1.0)\n",
        "            model.output.conv2D = nn.Conv2d(1280, 1, kernel_size=(1, 1), stride=(1, 1))\n",
        "          \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        x = self.arch(x)\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtSFM_tZKcna"
      },
      "source": [
        "class AverageMeter():\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjEEY5Pu3Ms-"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw4G_vuUGb3Z"
      },
      "source": [
        "def loss_fn_mse(outputs,targets):\n",
        "  criterion = nn.MSELoss()\n",
        "  loss = criterion(outputs,targets)\n",
        "  return loss "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDcMDds8thES"
      },
      "source": [
        "def loss_fn_mae(outputs,targets):\n",
        "  criterion = nn.L1Loss()\n",
        "  loss = criterion(outputs,targets)\n",
        "  return loss "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1BW4bwPB7qH"
      },
      "source": [
        "def loss_fn_smoothl1(outputs,targets):\n",
        "  criterion = nn.SmoothL1Loss()\n",
        "  loss = criterion(outputs,targets)\n",
        "  return loss "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ilJ_wZt4UVw"
      },
      "source": [
        "def train_fn(train_data_loader,model,optimizer,device,scheduler = None):\n",
        "  model.train()\n",
        "  losses = AverageMeter()\n",
        "  tk0 = tqdm(train_data_loader, total=len(train_data_loader))\n",
        "  tot_loss = 0\n",
        "  for bi,d in enumerate(tk0):\n",
        "    images = d['image']\n",
        "    labels = d['label']\n",
        "\n",
        "    #send them to device \n",
        "    images = images.to(device,dtype=torch.float)\n",
        "    labels = labels.to(device,dtype=torch.float)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs  = model(images)\n",
        "    \n",
        "    if LOSS == 'MSE':\n",
        "      loss = loss_fn_mse(outputs,labels.unsqueeze(1))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      tot_loss = tot_loss + loss.item()\n",
        "      losses.update(loss.item(), labels.size(0))\n",
        "      tk0.set_postfix(loss_mse=losses.avg)\n",
        "    elif LOSS == 'MAE':\n",
        "      loss = loss_fn_mae(outputs,labels.unsqueeze(1))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      tot_loss = tot_loss + loss.item()\n",
        "      losses.update(loss.item(), labels.size(0))\n",
        "      tk0.set_postfix(loss_mae=losses.avg)\n",
        "    elif LOSS == 'SMOOTH_L1':\n",
        "      loss = loss_fn_smoothl1(outputs,labels.unsqueeze(1))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      tot_loss = tot_loss + loss.item()\n",
        "      losses.update(loss.item(), labels.size(0))\n",
        "      tk0.set_postfix(loss_smooth_l1=losses.avg)\n",
        "\n",
        "    if scheduler is not None:\n",
        "      scheduler.step()\n",
        "  return losses.avg\n",
        "  "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk-5rsdI493E"
      },
      "source": [
        "def eval_fn(valid_data_loader,model,device):\n",
        "  model.eval()\n",
        "  tot_loss = 0\n",
        "  final_outputs = []\n",
        "  final_targets = []\n",
        "  with torch.no_grad():\n",
        "    for bi,d in enumerate(valid_data_loader):\n",
        "\n",
        "      images = d['image']\n",
        "      labels = d['label']\n",
        "\n",
        "      #send them to device \n",
        "      images = images.to(device,dtype=torch.float)\n",
        "      labels = labels.to(device,dtype=torch.float)\n",
        "     \n",
        "\n",
        "      outputs  = model(images)\n",
        "\n",
        "      if LOSS == 'MSE':\n",
        "        loss = loss_fn_mse(outputs,labels.unsqueeze(1))\n",
        "        tot_loss = tot_loss + loss.item()\n",
        "        \n",
        "      elif LOSS == 'MAE':\n",
        "        loss = loss_fn_mae(outputs,labels.unsqueeze(1))\n",
        "        tot_loss = tot_loss + loss.item()\n",
        "      elif LOSS == 'SMOOTH_L1':\n",
        "        loss = loss_fn_smoothl1(outputs,labels.unsqueeze(1))\n",
        "        tot_loss = tot_loss + loss.item()\n",
        "\n",
        "      \n",
        "      final_outputs.append(outputs.cpu().detach().numpy())\n",
        "      final_targets.append(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "    final_targets = np.concatenate(final_targets)\n",
        "    final_outputs = np.concatenate(final_outputs)\n",
        "    final_outputs = final_outputs.reshape(final_targets.shape )\n",
        "    mean_loss_val  = tot_loss/len(valid_data_loader)\n",
        "    rmse_score = np.sqrt(((final_targets - final_outputs) ** 2).mean())\n",
        "\n",
        "    print(f\"Validation loss {LOSS} for this epoch: \",mean_loss_val)\n",
        "    print('Validation rmse for this epoch',rmse_score)\n",
        "  return rmse_score\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxqJubi35FM1"
      },
      "source": [
        "input_shape = [128,256]\n",
        "def get_transforms():\n",
        "  val_transform = albumentations.Compose([\n",
        "      albumentations.Resize(input_shape[0],input_shape[1]),\n",
        "      albumentations.Normalize(),\n",
        "      AT.ToTensor()\n",
        "      ])\n",
        "  train_transform = albumentations.Compose([\n",
        "      albumentations.Resize(input_shape[0],input_shape[1]),\n",
        "      albumentations.Normalize(),\n",
        "      AT.ToTensor()\n",
        "      ])\n",
        "  return train_transform,val_transform"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ1uFXcKYttx"
      },
      "source": [
        "## Stratfied Kfolds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8votnEygYwg-",
        "outputId": "e4565345-5fcd-407b-99be-4c612d934ce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "NAME = 'resnext50'\n",
        "EPOCHS = 20\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "LR = 3e-4\n",
        "LOSS = 'MSE'\n",
        "skf = StratifiedKFold(n_splits=6,random_state=SEED_VAL)\n",
        "\n",
        "all_rmse_scores = []\n",
        "def run_folds():\n",
        "  seed_all(SEED_VAL)\n",
        "  train_transform,val_transform = get_transforms()\n",
        "  train = pd.read_csv('Train.csv')\n",
        "  train = train[train.label_quality == 2]\n",
        "  train = train.rename(columns = {'UID':'image_name','growth_stage':'target'})\n",
        "  DF = pd.DataFrame()\n",
        "  for i,(train_index,val_index) in enumerate(skf.split(train,y=train.target)):\n",
        "    print(f\"#########################  Fold {i+1}/{skf.n_splits}  #########################\")\n",
        "    train_df , valid_df = train.iloc[train_index,:],train.iloc[val_index,:]\n",
        "    train_df = train_df.reset_index()\n",
        "    valid_df = valid_df.reset_index()\n",
        "    train_dataset = ImageDataset('Images',train_df,train_transform)\n",
        "    valid_dataset = ImageDataset('Images',valid_df,val_transform)\n",
        "    train_data_loader = DataLoader(dataset=train_dataset,shuffle=True,batch_size=TRAIN_BATCH_SIZE)\n",
        "    valid_data_loader = DataLoader(dataset=valid_dataset,shuffle=False,batch_size=16)\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = Net(NAME)\n",
        "    model.to(device)\n",
        "    num_train_steps = int(len(train_df) /TRAIN_BATCH_SIZE * EPOCHS )\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "    best_rmse = 1500\n",
        "    \n",
        "    for epoch in range(EPOCHS):\n",
        "      print(\"----------------EPOCH \"+str(epoch+1)+\"---------------------\")\n",
        "      rmse_train = train_fn(train_data_loader, model, optimizer, device,scheduler=None)\n",
        "      rmse_val = eval_fn(valid_data_loader ,model, device)\n",
        "      if rmse_val<best_rmse:\n",
        "        best_rmse = rmse_val \n",
        "        torch.save(model.state_dict(),f\"best_model_{i}\") \n",
        "    print(f'best VAL_RMSE for fold {i+1}: ',best_rmse)\n",
        "    DF = DF.append({'FODL':1+i,'BEST_VAL_RMSE':best_rmse},ignore_index=True)\n",
        "    all_rmse_scores.append(best_rmse)\n",
        "  print(f\"MEAN RMSE: {np.mean(all_rmse_scores)}\")\n",
        "  return DF"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5iH5OW0a3zR"
      },
      "source": [
        "kfold_results = run_folds()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2TQ4tg9s5_O"
      },
      "source": [
        "kfold_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FieP1WYLMR0"
      },
      "source": [
        "# Predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juIpiQpwGXIZ"
      },
      "source": [
        "test_transform = albumentations.Compose([\n",
        "      albumentations.Resize(input_shape[0],input_shape[1]),\n",
        "      albumentations.Normalize(),\n",
        "      AT.ToTensor()\n",
        "      ])\n",
        "  \n",
        "test = pd.read_csv('SampleSubmission.csv')\n",
        "test.rename(columns={'UID':'image_name'},inplace=True)\n",
        "test_dataset = ImageDataset('Images',test,test_transform,mode='test')\n",
        "test_data_loader = DataLoader(dataset=test_dataset,shuffle=False,batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvraJ4kGbTyF"
      },
      "source": [
        "all_folds = []\n",
        "for i in range(skf.n_splits):\n",
        "  best_model = Net(NAME)\n",
        "  best_model.load_state_dict(torch.load(f'best_model_{i}'))\n",
        "  best_model.to(device)\n",
        "  best_model.eval()\n",
        "  final_outputs = []\n",
        "  with torch.no_grad():\n",
        "    tk0 = tqdm(test_data_loader, total=len(test_data_loader))\n",
        "    for bi,d in enumerate(tk0):\n",
        "      images = d['image']\n",
        "      #send them to device \n",
        "      images = images.to(device,dtype=torch.float)\n",
        "      outputs  = best_model(images)\n",
        "      final_outputs.append(outputs.cpu().detach().numpy())\n",
        "  final_outputs = np.concatenate(final_outputs)\n",
        "  all_folds.append(final_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMaqK3dlbTyT"
      },
      "source": [
        "ss = pd.read_csv('/content/SampleSubmission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQSY14qkbTyc"
      },
      "source": [
        "ss['growth_stage'] = np.mean(all_folds,axis=0)\n",
        "ss['growth_stage'] = ss['growth_stage'].clip(1,7)\n",
        "ss.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd1YgE9tbTyv"
      },
      "source": [
        "ss.to_csv(f'{NAME}_highQ_floats_Folds={skf.n_splits}input_shape=[{input_shape[0]}X{input_shape[1]}]_lr={LR}_epochs={EPOCHS}_bs={TRAIN_BATCH_SIZE}_SEED={SEED_VAL}.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgSMALakmS3F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}